# synth_gec_es
[![en](https://img.shields.io/badge/lang-en-red.svg)](README.md)
[![es](https://img.shields.io/badge/lang-es-yellow.svg)](README-es.md)

**Code for replication of fine-tuned models is in [./models](models/README.md).**

**version 0.6.2**

SYNTHetic Grammatical Error Correction for Spanish (ES) is a system for generating synthetic GEC data for common Spanish grammatical errors to train a GEC model.

To be used to augment smaller high-quality training sets as in *GECToR – Grammatical Error Correction: Tag, Not Rewrite* (2020)

**WORK IN PROGRESS**
All scripts are working, but may still contain minor bugs. In general, if a script fails, an error message will print and it will continue with the rest of the file, ignoring that sentence.

Required libraries for all scripts:
```
bs4 == 0.0.2
spacy == 3.7.6
unidecode == 1.3
transformers == 4.31.0
torch==2.4.1
numpy = 1.24.4
nltk == 3.8.1
```

## Table of Contents
* [Overview](#overview)
* [Scripts](#scripts)
  * [generate.py](#generatepy)
  * [decode.py](#decodepy)
  * [label.py](#labelpy)
* [Definitions](#definitions)
* [Mutation Types](#mutation-types)
* [Token-Level Stacking](#token-level-stacking)

## Overview

This repo contains a set of tool scripts to:
1. **Generate** synthetic errors in correct sentences in Spanish using [generate.py](#generatepy). Synthetic errors are generated using rule-based error definitions defined as JSON dicts. This script is designed to be somewhat modular to allow for additional error rules to be introduced or for specific errors to be generated more often. Additionally, there is the option of generating token-level transformations for each errorful + correct sentence pair generated by this script. The transformation definitions are described [below](#definitions).
2. **Decode** errorful sentences + token-level transformations into corrected sentences using [decode.py](#decodepy). Generally, this script would be useful for bulk decoding of output from a fine-tuned model trained on these token-level transformations.
3. **Label** errorful sentences + correct sentences with token-level transformations using [label.py](#labelpy). Use this if you have existing GEC training data, but would like to train your model for token classification rather than seq2seq.
4. Utility functions to easily incorporate this schema into a model in [utils.py](utils/utils.py), such as converting a string of token labels into a vector (and vice versa), decoding a single sentence, and morphology resolution (e.g. *estar* + MOOD-SUB + PERSON-2 + TIME-PRES = *estes*).

In addition, there are four fine-tuned models using this synthetic data available. Instructions for using these models are in [./models](models/README.md).

## Scripts

Note: If using the same file organization as the repo (i.e. if you cloned this repo), do not worry about the --dict_file, --vocab_file, --error_files, --spacy_model, or --tokenizer_model args.

### generate.py

The main script is generate.py. This script can be ran as:

```
python generate.py INPUT_FILE OUTPUT_FILE [--error_files] [ERROR_FILE_1 ERROR_FILE_2 ... ERROR_FILE_N] [-min/-min_error] [minimum number of errors in a sentence] [-max/--max_error] [maximum number of errors in a sentence] [-d/--dict_file] [dictionary file] [--vocab_file] [vocab file] [--spacy_model] [spaCy model name] [--tokenizer_model] [tokenizer model path] [--seed] [seed] [-n/--num-sentences] [number of sentences to generate for each original] [--n_cores] [number of cores to use] [-t/--token] [--verify] [-v/--verbose] [-sw/--silence_warnings] [--strict]
```

This script generates synthetic errorful sentences from well-formed Spanish sentences in a corpus.

* input file is a path to a file with one sentence in Spanish on a line with a blank line in between each sentence. Note that this script assumes unlabeled data and that each line contains only one sentence.
* output file is optional, defines the output path to place the generated data in. If no output path is supplied, it is placed in the same directory as the input file w/ the name [input file name]_synth.txt.
* --error_files are json files with lists of errors to apply
* --min_error is the minimum number of errors to be generated for a sentence
* --max_error is the maximum number of errors to be generated for a sentence
* --dict_file is the path to the dictionary file which supplies different morphological forms for a word
* --vocab_file is the path to the vocab file containing all words in your model's vocabulary
* --spacy_model is the spaCy model to use for tagging and morphology
* --tokenizer_model is the tokenizer model to use (either local or HuggingFace path)
* --seed is the seed for random generation
* --num-sentences is the number of errorful sentences that will be generated from each correct sentence in the supplied corpus. By default 1, but you can set it higher to generate more errorful sentences from one sentence with difference errors.
* --n_cores specifies the number of cores to use. If 1 (default), multi-processing will be disabled.
* --token means the output synthetic data will include token-level labels for the errorful sentence (discussed [below](#definitions)) for use in a token-level GEC system (as with GECToR)
* --verify means the generated token labels will be verified using the decode algorithm to ensure that the result matches the correct sentence. Has no effect if -t is not present. Note that this will generally double the time to label, but guarantees that the labels are valid. Currently, I recommend using --verify to ensure a high-quality dataset.
* --verbose means debugging code will print, NOT RECOMMENDED WITH MULTI-PROCESSING
* --silence_warnings means warnings will not be printed, such as a mutation being impossible which needed to use REPLACE as a fallback label.
* --strict means the script will be strict about which sentences are included in the output file. This has the primary effect of excluding sentences where a MUTATE verification failed and a REPLACE had to be used to repair the labels.

### decode.py

Ran as:

```
python3 decode.py INPUT_FILE [OUTPUT_FILE] [-d/--dict_file] [dictionary file] [--vocab_file] [vocab file] [--spacy_model] [spaCy model name] [--tokenizer_model] [tokenizer model path] [--n_cores] [number of cores to use] [-v/--verbose] [-sw/--silence_warnings]
```

This script takes errorful sentences + token-level labels and decodes them into a corrected sentence.

* input file is a path to a file with a sentence on one line, the respective token-level labels on the next line, and a blank line before the next sentence
* output file is optional, defines the output path to place the parsed sentences in. If no output path is supplied, it is placed in the same directory as the input file w/ the name [input file name]_parsed.txt.
* --dict_file is the path to the dictionary file which supplies different morphological forms for a word
* --vocab_file is the path to the vocab file containing all words in your model's vocabulary
* --spacy_model is the spaCy model to use for tagging and morphology
* --tokenizer_model is the tokenizer model to use (either local or HuggingFace path)
* --n_cores specifies the number of cores to use. If 1 (default), no multi-processing will be applied
* --verbose means debugging code will print, NOT RECOMMENDED WITH MULTI-PROCESSING
* --silence_warnings means warnings will not be printed, such as a mutation being replaced by a replace

### label.py

Ran as:

```
python3 label.py INPUT_FILE [OUTPUT_FILE] [-d/--dictionary-file] [dictionary file] [--vocab_file] [vocab file] [--spacy_model] [spaCy model name] [--tokenizer_model] [tokenizer model path] [--n_cores] [number of cores to use] [--verify] [-v/--verbose] [-sw/--silence_warnings] [--strict]
```

This script takes errorful sentences + target sentences and translates them into token-level edits using shortest edit distance.

* input file is a path to a file with an errorful sentence on one line, the target sentence on the next, and a blank line before the next sentence
* output file is optional, defines the output path to place the labeled sentences in. If no output path is supplied, it is placed in the same directory as the input file w/ the name [input file name]_labeled.txt.
* --dictionary-file is the path to the dictionary file which supplies different morphological forms for a word
* --vocab-file is the path to the vocab file containing all words in your model's vocabulary
* --spacy_model is the spaCy model to use for tagging and morphology
* --tokenizer_model is the tokenizer model to use (either local or HuggingFace path)
* --n_cores specifies the number of cores to use. If 1 (default), no multi-processing will be applied
* --verify means the generated token labels will be verified using the decode algorithm to ensure that the result matches the correct sentence. Note that this will generally double the time to label, but guarantees that the labels are valid
* --verbose means debugging code will print, NOT RECOMMENDED WITH MULTI-PROCESSING
* --silence_warnings means warnings will not be printed, such as a mutation being replaced by a replace
* --strict means the script will be strict about which sentences are included in the output file. This has the primary effect of excluding sentences where a MUTATE verification failed and a REPLACE had to be used to repair the labels.

## Definitions

NOTE: In order to accomodate adding words to the end of the sentence (because adds are prepends), sentences are appended with \[EOS\] and these tokens are given token-level labels. This is handled by the generation code, so you do not need to apply this yourself. However, when training a token classification model, an \[EOS\] token must be added to the end if not already done by your tokenizer.

The valid token-level labels are:
* `<KEEP/>`
  * No change to the token
* `<DELETE/>`
  * Delete the token
* `<ADD param=i/>`
  * Add the given token (based on token index in vocab.txt) immediately before this token
* `<COPY-REPLACE param=i/>`
  * Replaces this token with the given token (based on token index in the original sentence)
* `<COPY-ADD param=i/>`
  * Add the given token (based on token index in the original sentence) immediately before this token
* `<MUTATE param=i/>`
  * Mutate the morphology of this token based on a given type. Major types include CAPITALIZE, GENDER, NUMBER, PERSON, MOOD, and TIME. Further discussion [here](#mutation-types)
* `<REPLACE param=i/>`
  * Replace this token with the given token (based on token index in vocab.txt)

## Mutation Types

* CAPITALIZE
  * TRUE - capitalizes the word
  * FALSE - uncapitzalizes the word
* POS
  * Note: Because the morphology from different parts-of-speech are not generally compatible, default morphology is defined for each POS
  * NOUN - changes the part-of-speech to noun
    * Default morphology = SING, MASC
  * PRONOUN changes the part-of-speech to pronoun
    * Default morphology = SING, MASC, NOM
  * PERSONAL_PRONOUN changes the part-of-speech to personal pronoun
    * Default morphology = SING, MASC, NOM, BASE, NO
  * VERB changes the part-of-speech to verb
    * Default morphology = SING, IND, PRES, 3
  * ARTICLE changes the part-of-speech to article
    * Default morphology = SING, MASC, DEF
  * ADJ changes the part-of-speech to adjective
    * Default morphology = SING, MASC
  * ADV changes the part-of-speech to adverb
    * Default morphology = SING, MASC
* GENDER (GÉNERO)
  * MASC - makes the word masculine (if possible)
    * e.g. apuesta + `<MUTATE param=GENDER-MASC/>` = apuesto
  * FEM - makes the word feminine (if possible)
    * e.g. harto + `<MUTATE param=GENDER-MASC/>` = harta
* NUMBER (NÚMERO)
  * SING - makes the word singular
  * PLU - makes the word plural
* DEFINITE
  * DEF - makes the word definite
  * IND - makes the word indefinite
* CASE
  * NOM - makes the word nominative
  * ACC - makes the word accusative
  * DAT - makes the word dative
* PRONOUN_TYPE (for personal pronouns only)
  * BASE - makes the pronoun of the normal base type (e.g. *él*, *yo*, *usted*)
  * CLITIC - makes the pronoun of the clitic type (e.g. *le*, *se*, te*, *me*)
* REFLEXIVE (for personal pronouns only and only affects clitic pronouns)
  * YES - makes the pronoun reflexive (e.g. *se*, *me*, *te*)
  * NO - makes the pronoun non-reflexive (e.g. *le*, *se*, te*, *me*)
* PERSON (PERSONA)
  * 1 - makes the word first-person (must be verb)
    * e.g. son + `<MUTATE type=PERSON-1` = somos
  * 2 - makes the word second-person (must be verb)
  * 3 - makes the word third-person (must be verb)
* MOOD (MODO)
  * IND (INDICATIVO) - makes the word indicative in mood (must be verb)
  * SUB (SUBJUNTIVO) - makes the word subjunctive in mood (must be verb)
  * **DEPRECATED**[^1] PROG (PROGRESIVO) - makes the word progressive in mood (must be verb) (adds the correct conjugation of *estar* before the verb)
  * **DEPRECATED**[^1] PERF (PERFECTO) - makes the word perfect in mood (must be verb) (adds the correct conjugation of *haber* before the verb)
  * **DEPRECATED**[^1] PERF-SUBJ (PERFECTO SUBJUNCTIVO) - makes the word perfect subjunctive in mood (must be verb) (adds the correct conjugation of *haber* before the verb)
  * GER (PARTICIPIO PRESENTE) - makes the word in the gerund form, distinct from progressive which also adds the correct conjugation of *estar* (must be verb)
  * PAST-PART (PARTICIPIO PASADO) - makes the word in the past participle form (must be verb)
  * INF (INFINITIVO) - makes word in the infinitive form (i.e. no mood) (must be verb)
* TIME (TIEMPO)
  * PRES (PRESENTE)
  * PRET (PRETÉTERITO)
  * IMP (IMPERFECTO)
  * CND[^2] (CONDICIONAL)
  * FUT (FUTURO)

  [^1]: Mutations which require adding extra words are deprecated. The functionality remains in decode.py, but these types of mutations will not result from automatically generated labels from generate.py or label.py.
  [^2]: Note that the conditional is considered a tense. There is some disagreement over whether the conditional is a mood or a tense in Spanish, but I consider it a tense in my label schema.

## Token-Label Stacking
You may have noticed that these labels are not mutually exclusive. Some labels are incompatible or redundant together, but many would be expected to operate in conjunction. Therefore, the textual label is formatted with tabs in between each token's label. If there is no tab between two labels, then all of those labels apply to the same token. For example:

```
espero que corre tú bien.
<MUTATE param="CAPITALIZE-TRUE"/>  <KEEP/>  <MUTATE param="PERSON-2"/> <MUTATE param="MOOD-SUBJ"/> <COPY-ADD param=3/> <DELETE/>  <KEEP/>  <KEEP/>
```

changes the sentence to:

`Espero que tú corras bien.`

This means that this defines a *multi-label classification task* for each token. The expected output for each token is an integer vector of length max_labels_per_token * 2, where each individual token label is defined with two integers, the first defining the main label type (e.g. KEEP, REPLACE, MUTATE) and the second defining the parameter (e.g. MOOD-SUB, index 1, index 1,243). The token-labels from the vector are applied from the beginning of the vector to the end.

For example, let's define the following transformation:

Token: `estás`

Token-labels: `<MUTATE param="PERSON-3"/> <MUTATE param="MOOD-SUB"/> <MUTATE param="TIME-PRET"/>`

For convenience, the numerical representations of each of property in these labels are:
* MUTATE = 5
* PERSON-3 = 24
* MOOD-SUB = 26
* TIME-PRET = 31

With max_labels_per_token = 5, the vector representation of these labels is:

`[5, 24, 5, 26, 5, 31, 0, 0, 0, 0]`

and this results in the transformed token: `estuviera`

Note this is still a WIP, so I'm happy to take feedback for adjustments of this formatting.