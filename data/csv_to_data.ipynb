{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec848b0-b01d-48b6-b036-ef95ef859ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa023b3-a4ae-4b7e-8d59-5af923da495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file system\n",
    "\n",
    "input_dir = \"COWS-L2H-CSV\"\n",
    "output_dir = \".\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db65984a-87f0-4319-9773-7694d3619adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=[\"essay\", \"corrected1\"])\n",
    "for file in Path(input_dir).glob(\"*.csv\"):\n",
    "    cur_df = pd.read_csv(file)\n",
    "    cur_df = cur_df[[\"essay\", \"corrected1\"]]\n",
    "    df = pd.concat((df, cur_df))\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1660e51f-b6a2-460a-a4d2-e3688e1fecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad676f8-0465-45a3-b125-4975d619194a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seems_like(text1, text2):\n",
    "    '''\n",
    "    Defined as having more than 50% lemma overlap or the sentences are shorter than 4 tokens, naive and quick approach\n",
    "    '''\n",
    "    try:\n",
    "        words1 = {token.lemma_ for token in text1}\n",
    "    except AttributeError:\n",
    "        words1 = {token.lemma_ for token in nlp(text1)}\n",
    "    try:\n",
    "        words2 = {token.lemma_ for token in text2}\n",
    "    except AttributeError:\n",
    "        words2 = {token.lemma_ for token in nlp(text2)}\n",
    "    ratio = len(words1 & words2) / min(len(words1), len(words2))\n",
    "    return ratio >= 0.5 or (len(words1) < 4 and len(words2) < 4)\n",
    "    \n",
    "def row_to_sentence_pairs(row):\n",
    "    global skipped_sentences\n",
    "\n",
    "    clean_errorful = re.sub(r\"\\s\", ' ', row[\"essay\"]).strip()\n",
    "    clean_corrected = re.sub(r\"\\s\", ' ', row[\"corrected1\"]).strip()\n",
    "    \n",
    "    replacements = [(r\"\\*AGE\\*\", \"edad\"), (r\"\\*CITY\\*\", \"ciudad\"), (r\"\\*STATE\\*\", \"estado\"), \\\n",
    "                    (r\"\\*BIRTH_DATE\\*\", \"fecha de nacimiento\"), (r\"\\*UNIVERSITY\\*\", \"universidad\"), (r\"\\*PLACE\\*\", \"lugar\"), \\\n",
    "                   (r\"\\*FIRST_NAME\\*\", \"nombre\"), (r\"\\*LAST_NAME\\*\", \"apellido\"), (r\"\\*NUMBER\\*\", \"nÃºmero\")]\n",
    "    for replacement in replacements:\n",
    "        clean_errorful = re.sub(replacement[0], replacement[1], clean_errorful)\n",
    "        clean_corrected = re.sub(replacement[0], replacement[1], clean_corrected)\n",
    "    if len(re.sub(r\"\\*\", '', clean_errorful)) != len(clean_errorful): # Check if there are any remaining special tokens\n",
    "        print(clean_errorful)\n",
    "    errorful_sentences = list(nlp(clean_errorful).sents)\n",
    "    corrected_sentences = list(nlp(clean_corrected).sents)\n",
    "\n",
    "    paired = []\n",
    "    i = 0\n",
    "    j = 0\n",
    "    while i < len(errorful_sentences) or j < len(corrected_sentences):\n",
    "        if i >= len(errorful_sentences):\n",
    "            paired[-1] = (paired[-1][0], f\"{paired[-1][1]} {corrected_sentences[j]}\")\n",
    "            j += 1\n",
    "        elif j >= len(corrected_sentences):\n",
    "            paired[-1] = (f\"{paired[-1][0]} {errorful_sentences[i]}\", paired[-1][1])\n",
    "            i += 1\n",
    "        else:\n",
    "            # Handle empty strs\n",
    "            if re.sub(\"\\s\", '', str(errorful_sentences[i])) == \"\" and re.sub(\"\\s\", '', str(corrected_sentences[j])) == \"\":\n",
    "                i += 1\n",
    "                j += 1\n",
    "                continue\n",
    "            elif re.sub(\"\\s\", '', str(errorful_sentences[i])) == \"\":\n",
    "                i += 1\n",
    "                continue\n",
    "            elif re.sub(\"\\s\", '', str(corrected_sentences[j])) == \"\":\n",
    "                j += 1\n",
    "                continue\n",
    "            \n",
    "            if seems_like(errorful_sentences[i], corrected_sentences[j]) or len(paired) < 1:\n",
    "                paired.append((errorful_sentences[i], corrected_sentences[j]))\n",
    "                i += 1\n",
    "                j += 1\n",
    "            else:\n",
    "                if seems_like(errorful_sentences[i], paired[-1][1]):\n",
    "                    paired[-1] = (f\"{paired[-1][0]} {errorful_sentences[i]}\", paired[-1][1])\n",
    "                    i += 1\n",
    "                elif seems_like(corrected_sentences[j], paired[-1][0]):\n",
    "                    paired[-1] = (paired[-1][0], f\"{paired[-1][1]} {corrected_sentences[j]}\")\n",
    "                    j += 1\n",
    "                else:\n",
    "                    print(\"====================\\nFailure to resolve sentence position. Ignoring it.\")\n",
    "                    print(f\"Current Errorful Candidate: {errorful_sentences[i]}\")\n",
    "                    print(f\"Current Corrected Candidate: {corrected_sentences[j]}\")\n",
    "                    print(f\"Last Errorful Sentence: {paired[-1][0]}\")\n",
    "                    print(f\"Last Corrected Sentence: {paired[-1][1]}\")\n",
    "                    skipped_sentences += 1\n",
    "                    i += 1\n",
    "                    j += 1\n",
    "    print(f\"Skipped sentences: {skipped_sentences}\")\n",
    "    return paired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d86f466-a162-4cc0-8bd0-b0a7d5583767",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"es_dep_news_trf\")\n",
    "skipped_sentences = 0\n",
    "sentences = df.apply(row_to_sentence_pairs, axis=1)\n",
    "print(f\"Skipped sentences: {skipped_sentences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df375225-4722-4e0e-a341-9327e0eeb15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_pairs = [sentence_pair for group in sentences for sentence_pair in group]\n",
    "with open(Path(output_dir) / \"COWS-L2H-sentence-pairs.txt\", 'w') as f:\n",
    "    for sentence_pair in sentence_pairs:\n",
    "        f.write(f\"{sentence_pair[0]}\\n{sentence_pair[1]}\\n\\n\")\n",
    "with open(Path(output_dir) / \"COWS-L2H-only-corrected.txt\", 'w') as f:\n",
    "    for sentence_pair in sentence_pairs:\n",
    "        f.write(f\"{sentence_pair[1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b69106d-0222-4418-8784-3ea843305e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For seq2seq model\n",
    "\n",
    "with open(Path(output_dir) / \"COWS-L2H-unlabeled-STRICT.txt\", 'w') as out_f:\n",
    "    with open(Path(output_dir) / \"COWS-L2H-labeled-STRICT.txt\", 'r') as in_f:\n",
    "        lines = in_f.readlines()\n",
    "        for i in range(0, len(lines), 4):\n",
    "            errorful_sentence = lines[i]\n",
    "            token_labels = lines[i+1]\n",
    "            correct_sentence = lines[i+2]\n",
    "\n",
    "            out_f.write(f\"{errorful_sentence}{correct_sentence}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
